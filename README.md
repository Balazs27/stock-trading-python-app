# üìà Stock Trading Data Pipeline (Polygon API ‚Üí Snowflake)

## Project Overview

This project is a **practice data pipeline** built to develop hands-on experience with data ingestion, persistence, and warehouse loading using **real-world financial data**.

The pipeline fetches stock ticker data from the **Polygon API** and evolves through three ingestion stages:
1. API ‚Üí CSV  
2. API ‚Üí Snowflake  
3. API ‚Üí Snowflake with a date (`ds`) column for historical tracking  

The goal is to simulate realistic data engineering workflows while keeping the implementation simple and transparent.

---

## Pipeline Flow

Polygon API
‚Üì
Python script
‚Üì
(CSV file - first iteration)
‚Üì
Snowflake table
‚Üì
(Snowflake table with ds (date partition) - last iteration)

---

## Project Structure

| File | Description |
|-----|-------------|
| `script.py` | Fetches stock ticker data from Polygon API and writes it to a CSV file |
| `script_snowflake.py` | Fetches stock ticker data from Polygon API and loads it into Snowflake (basic version, no date column) |
| `script_snowflake_ds.py` | Fetches stock ticker data from Polygon API and loads it into Snowflake with an added `ds` (current date) column |
| `scheduler.py` | Demonstrates how the pipeline could be scheduled to run periodically |
| `tickers.csv` | Example output generated by the pipeline |
| `requirements.txt` | Python dependencies |
| `.env` | API keys & Snowflake credentials (not committed) |

---

## Pipeline Iterations

### 1Ô∏è‚É£ API ‚Üí CSV
- Fetches stock ticker data from Polygon API
- Stores results locally in a CSV file

### 2Ô∏è‚É£ API ‚Üí Snowflake
- Fetches stock ticker data from Polygon API and loads it into a Snowflake table

### 3Ô∏è‚É£ API ‚Üí Snowflake with `ds`
- Adds a `ds` column with the current date
- Enables:
  - Partitioning
  - Historical tracking
  - Change analysis over time

This mirrors common real-world patterns used in data / analytics engineering.

---

## Scheduling Concept

The project includes a simple example using the `schedule` library to demonstrate how the pipeline could be run:
- Every X minutes
- Hourly
- Daily

This is a conceptual step toward orchestration tools like cron, Airflow, or Prefect.

---

## Secrets Management

Sensitive credentials (Polygon API key, Snowflake credentials) are stored in a `.env` file and excluded from version control via `.gitignore`.

---

## Tech Stack

- Polygon API
- Python
- Snowflake

---

## Purpose

This project was created to:
- Practice building data pipelines with real APIs
- Learn Snowflake ingestion patterns
- Prepare for data engineering / analytics engineering roles
- Build a clean and simple, portfolio-ready example project

---

## Possible Extensions

- Incremental loading logic
- Error handling & logging
- Snowflake transformations
- dbt models (bronze / silver / gold)
- Workflow orchestration
